{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to S3 with a SageMaker Processing Job\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Quick Start </strong>\n",
    "To save your processed data to S3, select the Run menu above and click <strong>Run all cells</strong>. \n",
    "<strong><a style=\"color: #0397a7 \" href=\"#Job-Status-&-S3-Output-Location\">\n",
    "    <u>View the status of the export job and the output S3 location</u></a>.\n",
    "</strong>\n",
    "</div>\n",
    "\n",
    "\n",
    "This notebook executes your Data Wrangler Flow `1_features_engr_v1.flow` on the entire dataset using a SageMaker \n",
    "Processing Job and will save the processed data to S3.\n",
    "\n",
    "This notebook saves data from the step `Cast Single Data Type` from `Source: Airot2007All.Csv`. To save from a different step, go to Data Wrangler \n",
    "to select a new step to export. \n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Inputs and Outputs](#Inputs-and-Outputs)\n",
    "1. [Run Processing Job](#Run-Processing-Job)\n",
    "   1. [Job Configurations](#Job-Configurations)\n",
    "   1. [Create Processing Job](#Create-Processing-Job)\n",
    "   1. [Job Status & S3 Output Location](#Job-Status-&-S3-Output-Location)\n",
    "1. [Optional Next Steps](#(Optional)Next-Steps)\n",
    "    1. [Load Processed Data into Pandas](#(Optional)-Load-Processed-Data-into-Pandas)\n",
    "    1. [Train a model with SageMaker](#(Optional)Train-a-model-with-SageMaker)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Outputs\n",
    "\n",
    "The below settings configure the inputs and outputs for the flow export.\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Configurable Settings </strong>\n",
    "\n",
    "In <b>Input - Source</b> you can configure the data sources that will be used as input by Data Wrangler\n",
    "\n",
    "1. For S3 sources, configure the source attribute that points to the input S3 prefixes\n",
    "2. For all other sources, configure attributes like query_string, database in the source's \n",
    "<b>DatasetDefinition</b> object.\n",
    "\n",
    "If you modify the inputs the provided data must have the same schema and format as the data used in the Flow. \n",
    "You should also re-execute the cells in this section if you have modified the settings in any data sources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "\n",
    "data_sources = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input - S3 Source: airOT2007all.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources.append(ProcessingInput(\n",
    "    source=\"s3://from-public-data/carrier-perf/transformed/airOT2007all.csv\", # You can override this to point to other dataset on S3\n",
    "    destination=\"/opt/ml/processing/airOT2007all.csv\",\n",
    "    input_name=\"airOT2007all.csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: S3 settings\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Configurable Settings </strong>\n",
    "\n",
    "1. <b>bucket</b>: you can configure the S3 bucket where Data Wrangler will save the output. The default bucket from \n",
    "the SageMaker notebook session is used. \n",
    "2. <b>flow_export_id</b>: A randomly generated export id. The export id must be unique to ensure the results do not \n",
    "conflict with other flow exports \n",
    "3. <b>s3_ouput_prefix</b>:  you can configure the directory name in your bucket where your data will be saved.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Wrangler export storage bucket: sagemaker-us-west-2-506926764659\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "import sagemaker\n",
    "\n",
    "# Sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# You can configure this with your own bucket name, e.g.\n",
    "# bucket = \"my-bucket\"\n",
    "bucket = sess.default_bucket()\n",
    "print(f\"Data Wrangler export storage bucket: {bucket}\")\n",
    "\n",
    "# unique flow export ID\n",
    "flow_export_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the inputs required by the SageMaker Python SDK to launch a processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow S3 export result path: s3://sagemaker-us-west-2-506926764659/export-flow-05-16-30-08-0c003aed/output\n"
     ]
    }
   ],
   "source": [
    "# Output name is auto-generated from the select node's ID + output name from the flow file.\n",
    "output_name = \"b98f4f8c-ddaf-4ee1-99da-b0dd09f47a21.default\"\n",
    "\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "print(f\"Flow S3 export result path: {s3_output_path}\")\n",
    "\n",
    "processing_job_output = ProcessingOutput(\n",
    "    output_name=output_name,\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=s3_output_path,\n",
    "    s3_upload_mode=\"EndOfJob\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Flow to S3\n",
    "\n",
    "To use the Data Wrangler as an input to the processing job,  first upload your flow file to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flow file from current notebook working directory: /root/Untitled Folder\n",
      "Data Wrangler flow 1_features_engr_v1.flow uploaded to s3://sagemaker-us-west-2-506926764659/data_wrangler_flows/flow-05-16-30-08-0c003aed.flow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"1_features_engr_v1.flow\"\n",
    "\n",
    "# Load .flow file from current notebook working directory \n",
    "!echo \"Loading flow file from current notebook working directory: $PWD\"\n",
    "\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Upload flow to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"data_wrangler_flows/{flow_export_name}.flow\")\n",
    "\n",
    "flow_s3_uri = f\"s3://{bucket}/data_wrangler_flows/{flow_export_name}.flow\"\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Wrangler Flow is also provided to the Processing Job as an input source which we configure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input - Flow: 1_features_engr_v1.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=\"/opt/ml/processing/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Processing Job \n",
    "## Job Configurations\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Configurable Settings </strong>\n",
    "\n",
    "You can configure the following settings for Processing Jobs. If you change any configurations you will \n",
    "need to re-execute this and all cells below it by selecting the Run menu above and click \n",
    "<b>Run Selected Cells and All Below</b>\n",
    "\n",
    "1. IAM role for executing the processing job. \n",
    "2. A unique name of the processing job. Give a unique name every time you re-execute processing jobs\n",
    "3. Data Wrangler Container URL.\n",
    "4. Instance count, instance type and storage volume size in GB.\n",
    "5. Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "6. Network Isolation settings\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name AmazonSageMaker-ExecutionRole-20210503T205912 to get Role path.\n",
      "Assuming role was created in SageMaker AWS console, as the name contains `AmazonSageMaker-ExecutionRole`. Defaulting to Role ARN with service-role in path. If this Role ARN is incorrect, please add IAM read permissions to your role or supply the Role Arn directly.\n"
     ]
    }
   ],
   "source": [
    "# IAM role for executing the processing job.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Unique processing job name. Give a unique name every time you re-execute processing jobs\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n",
    "\n",
    "# Data Wrangler Container URL.\n",
    "container_uri = \"174368400705.dkr.ecr.us-west-2.amazonaws.com/sagemaker-data-wrangler-container:1.x\"\n",
    "\n",
    "# Processing Job Instance count and instance type.\n",
    "instance_count = 2\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Size in GB of the EBS volume to use for storing data during processing\n",
    "volume_size_in_gb = 30\n",
    "\n",
    "# Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# Network Isolation mode; default is off\n",
    "enable_network_isolation = False\n",
    "\n",
    "# Output configuration used as processing job container arguments \n",
    "output_config = {\n",
    "    output_name: {\n",
    "        \"content_type\": output_content_type\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processing Job\n",
    "\n",
    "To launch a Processing Job, you will use the SageMaker Python SDK to create a Processor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  data-wrangler-flow-processing-05-16-30-08-0c003aed\n",
      "Inputs:  [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-506926764659/data_wrangler_flows/flow-05-16-30-08-0c003aed.flow', 'LocalPath': '/opt/ml/processing/flow', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'airOT2007all.csv', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://from-public-data/carrier-perf/transformed/airOT2007all.csv', 'LocalPath': '/opt/ml/processing/airOT2007all.csv', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'b98f4f8c-ddaf-4ee1-99da-b0dd09f47a21.default', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-506926764659/export-flow-05-16-30-08-0c003aed/output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=volume_size_in_gb,\n",
    "    network_config=NetworkConfig(enable_network_isolation=enable_network_isolation),\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# Start Job\n",
    "processor.run(\n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[processing_job_output],\n",
    "    arguments=[f\"--output-config '{json.dumps(output_config)}'\"],\n",
    "    wait=False,\n",
    "    logs=False,\n",
    "    job_name=processing_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Status & S3 Output Location\n",
    "\n",
    "Below you wait for processing job to finish. If it finishes successfully, the raw parameters used by the \n",
    "Processing Job will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job results are saved to S3 path: s3://sagemaker-us-west-2-506926764659/export-flow-05-16-30-08-0c003aed/output/data-wrangler-flow-processing-05-16-30-08-0c003aed\n",
      "...........................................................................................!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'flow',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-506926764659/data_wrangler_flows/flow-05-16-30-08-0c003aed.flow',\n",
       "    'LocalPath': '/opt/ml/processing/flow',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'airOT2007all.csv',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://from-public-data/carrier-perf/transformed/airOT2007all.csv',\n",
       "    'LocalPath': '/opt/ml/processing/airOT2007all.csv',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'b98f4f8c-ddaf-4ee1-99da-b0dd09f47a21.default',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-506926764659/export-flow-05-16-30-08-0c003aed/output',\n",
       "     'LocalPath': '/opt/ml/processing/output',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'data-wrangler-flow-processing-05-16-30-08-0c003aed',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2,\n",
       "   'InstanceType': 'ml.m5.4xlarge',\n",
       "   'VolumeSizeInGB': 30}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       " 'AppSpecification': {'ImageUri': '174368400705.dkr.ecr.us-west-2.amazonaws.com/sagemaker-data-wrangler-container:1.x',\n",
       "  'ContainerArguments': ['--output-config \\'{\"b98f4f8c-ddaf-4ee1-99da-b0dd09f47a21.default\": {\"content_type\": \"CSV\"}}\\'']},\n",
       " 'NetworkConfig': {'EnableInterContainerTrafficEncryption': False,\n",
       "  'EnableNetworkIsolation': False},\n",
       " 'RoleArn': 'arn:aws:iam::506926764659:role/service-role/AmazonSageMaker-ExecutionRole-20210503T205912',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:506926764659:processing-job/data-wrangler-flow-processing-05-16-30-08-0c003aed',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ProcessingEndTime': datetime.datetime(2021, 5, 5, 16, 37, 42, 243000, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2021, 5, 5, 16, 34, 35, 544000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 5, 5, 16, 37, 42, 250000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2021, 5, 5, 16, 30, 9, 621000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '65be6f73-b7ad-466c-88a2-5a7d177441ac',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '65be6f73-b7ad-466c-88a2-5a7d177441ac',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2064',\n",
       "   'date': 'Wed, 05 May 2021 16:37:45 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_job_results_path = f\"s3://{bucket}/{s3_output_prefix}/{processing_job_name}\"\n",
    "print(f\"Job results are saved to S3 path: {s3_job_results_path}\")\n",
    "\n",
    "job_result = sess.wait_for_processing_job(processing_job_name)\n",
    "job_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional)Next Steps\n",
    "\n",
    "Now that data is available on S3 you can use other SageMaker components that take S3 URIs as input such as \n",
    "SageMaker Training, Built-in Algorithms, etc. Similarly you can load the dataset into a Pandas dataframe \n",
    "in this notebook for further inspection and work. The examples below show how to do both of these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default optional steps do not run automatically, set `run_optional_steps` to True if you want to \n",
    "execute optional steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_optional_steps = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_optional_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-16b269df4764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This will stop the below cells from executing if \"Run All Cells\" was used on the notebook.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrun_optional_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stop here. Do not automatically execute optional steps.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_optional_steps' is not defined"
     ]
    }
   ],
   "source": [
    "# This will stop the below cells from executing if \"Run All Cells\" was used on the notebook.\n",
    "if not run_optional_steps:\n",
    "    raise SystemExit(\"Stop here. Do not automatically execute optional steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Load Processed Data into Pandas\n",
    "\n",
    "We use the [AWS Data Wrangler library](https://github.com/awslabs/aws-data-wrangler) to load the exported \n",
    "dataset into a Pandas dataframe for a preview of first 1000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n"
     ]
    }
   ],
   "source": [
    "!pip install -q awswrangler pandas\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>UNIQUE_CARRIER</th>\n",
       "      <th>ORIGIN_AIRPORT_ID</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST_AIRPORT_ID</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>...</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>late_flight</th>\n",
       "      <th>DEP_DELAY_no_outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9E</td>\n",
       "      <td>11278</td>\n",
       "      <td>DCA</td>\n",
       "      <td>11986</td>\n",
       "      <td>GRR</td>\n",
       "      <td>2044.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>late</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9E</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>11003</td>\n",
       "      <td>CID</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>late</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9E</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>11637</td>\n",
       "      <td>FAR</td>\n",
       "      <td>913.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>late</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9E</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>10469</td>\n",
       "      <td>AZO</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>late</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9E</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>11298</td>\n",
       "      <td>DFW</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>852.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>late</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9E</td>\n",
       "      <td>12264</td>\n",
       "      <td>IAD</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>628.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>late</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9E</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>11076</td>\n",
       "      <td>CMX</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>late</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9E</td>\n",
       "      <td>14730</td>\n",
       "      <td>SDF</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>1848.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>late</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9E</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>13029</td>\n",
       "      <td>LNK</td>\n",
       "      <td>2150.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>late</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9E</td>\n",
       "      <td>13029</td>\n",
       "      <td>LNK</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>late</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     YEAR  MONTH  DAY_OF_MONTH  DAY_OF_WEEK UNIQUE_CARRIER  ORIGIN_AIRPORT_ID  \\\n",
       "0    2007      1             1            1             9E              11278   \n",
       "1    2007      1             1            1             9E              13487   \n",
       "2    2007      1             1            1             9E              13487   \n",
       "3    2007      1             1            1             9E              13487   \n",
       "4    2007      1             1            1             9E              13487   \n",
       "..    ...    ...           ...          ...            ...                ...   \n",
       "995  2007      1             2            2             9E              12264   \n",
       "996  2007      1             2            2             9E              13487   \n",
       "997  2007      1             2            2             9E              14730   \n",
       "998  2007      1             2            2             9E              13487   \n",
       "999  2007      1             2            2             9E              13029   \n",
       "\n",
       "    ORIGIN  DEST_AIRPORT_ID DEST  DEP_TIME  ...  ARR_DELAY  AIR_TIME  \\\n",
       "0      DCA            11986  GRR    2044.0  ...       70.0      95.0   \n",
       "1      MSP            11003  CID    1939.0  ...       75.0      39.0   \n",
       "2      MSP            11637  FAR     913.0  ...      -19.0      39.0   \n",
       "3      MSP            10469  AZO    2122.0  ...       53.0      55.0   \n",
       "4      MSP            11298  DFW    1410.0  ...       -7.0     123.0   \n",
       "..     ...              ...  ...       ...  ...        ...       ...   \n",
       "995    IAD            13487  MSP     628.0  ...        4.0     154.0   \n",
       "996    MSP            11076  CMX    2140.0  ...       -5.0      39.0   \n",
       "997    SDF            13487  MSP    1848.0  ...       -5.0     101.0   \n",
       "998    MSP            13029  LNK    2150.0  ...       11.0      53.0   \n",
       "999    LNK            13487  MSP    1300.0  ...      -16.0      52.0   \n",
       "\n",
       "     DISTANCE  CARRIER_DELAY  WEATHER_DELAY  NAS_DELAY  SECURITY_DELAY  \\\n",
       "0       524.0            0.0            0.0       26.0             0.0   \n",
       "1       221.0            7.0            0.0       68.0             0.0   \n",
       "2       223.0            NaN            NaN        NaN             NaN   \n",
       "3       426.0           53.0            0.0        0.0             0.0   \n",
       "4       852.0            NaN            NaN        NaN             NaN   \n",
       "..        ...            ...            ...        ...             ...   \n",
       "995     908.0            NaN            NaN        NaN             NaN   \n",
       "996     277.0            NaN            NaN        NaN             NaN   \n",
       "997     603.0            NaN            NaN        NaN             NaN   \n",
       "998     331.0            NaN            NaN        NaN             NaN   \n",
       "999     331.0            NaN            NaN        NaN             NaN   \n",
       "\n",
       "     LATE_AIRCRAFT_DELAY  late_flight  DEP_DELAY_no_outlier  \n",
       "0                   44.0         late                    44  \n",
       "1                    0.0         late                     7  \n",
       "2                    NaN         late                    -2  \n",
       "3                    0.0         late                    57  \n",
       "4                    NaN         late                    10  \n",
       "..                   ...          ...                   ...  \n",
       "995                  NaN         late                    -2  \n",
       "996                  NaN         late                     0  \n",
       "997                  NaN         late                    -1  \n",
       "998                  NaN         late                     5  \n",
       "999                  NaN         late                    -5  \n",
       "\n",
       "[1000 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize = 1000\n",
    "\n",
    "if output_content_type.upper() == \"CSV\":\n",
    "    dfs = wr.s3.read_csv(s3_output_path, chunksize=chunksize)\n",
    "elif output_content_type.upper() == \"PARQUET\":\n",
    "    dfs = wr.s3.read_parquet(s3_output_path, chunked=chunksize)\n",
    "else:\n",
    "    print(f\"Unexpected output content type {output_content_type}\") \n",
    "\n",
    "df = next(dfs)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional)Train a model with SageMaker\n",
    "Now that the data has been processed, you may want to train a model using the data. The following shows an \n",
    "example of doing so using a popular algorithm - XGBoost. For more information on algorithms available in \n",
    "SageMaker, see [Getting Started with SageMaker Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html). \n",
    "It is important to note that the following XGBoost objective ['binary', 'regression', 'multiclass'] \n",
    "hyperparameters, or content_type may not be suitable for the output data, and will require changes to \n",
    "train a proper model. Furthermore, for CSV training, the algorithm assumes that the target \n",
    "variable is in the first column. For more information on SageMaker XGBoost, \n",
    "see https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html.\n",
    "\n",
    "\n",
    "### Set Training Data path\n",
    "We set the training input data path from the output of the Data Wrangler processing job.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training input data path: s3://sagemaker-us-west-2-506926764659/export-flow-05-16-30-08-0c003aed/output/data-wrangler-flow-processing-05-16-30-08-0c003aed\n"
     ]
    }
   ],
   "source": [
    "s3_training_input_path = s3_job_results_path\n",
    "print(f\"training input data path: {s3_training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the algorithm and training job\n",
    "\n",
    "The Training Job hyperparameters are set. For more information on XGBoost Hyperparameters, \n",
    "see https://xgboost.readthedocs.io/en/latest/parameter.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "hyperparameters = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"10\",\n",
    "}\n",
    "train_content_type = (\n",
    "    \"application/x-parquet\" if output_content_type.upper() == \"PARQUET\"\n",
    "    else \"text/csv\"\n",
    ")\n",
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=s3_training_input_path,\n",
    "    content_type=train_content_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Training Job\n",
    "\n",
    "The TrainingJob configurations are set using the SageMaker Python SDK Estimator, and which is fit using \n",
    "the training data from the Processing Job that was run earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-05 16:41:42 Starting - Starting the training job...\n",
      "2021-05-05 16:42:05 Starting - Launching requested ML instancesProfilerReport-1620232902: InProgress\n",
      "............\n",
      "2021-05-05 16:44:06 Starting - Preparing the instances for training......\n",
      "2021-05-05 16:45:06 Downloading - Downloading input data...\n",
      "2021-05-05 16:45:37 Training - Training image download completed. Training in progress..\u001b[34m[2021-05-05 16:45:39.136 ip-10-0-139-228.us-west-2.compute.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 7294650 rows and 22 columns\u001b[0m\n",
      "\u001b[34m[16:45:45] WARNING: ../src/gbm/gbtree.cc:139: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:1402.38147\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:983.63379\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:684.84979\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:480.03772\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:338.40643\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:236.71408\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:164.87949\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:115.45071\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:81.31853\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:56.87903\u001b[0m\n",
      "\n",
      "2021-05-05 16:46:26 Uploading - Uploading generated training model\n",
      "2021-05-05 16:46:26 Completed - Training job completed\n",
      "Training seconds: 87\n",
      "Billable seconds: 87\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    iam_role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    ")\n",
    "estimator.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained model there are a number of different things you can do. \n",
    "For more details on training with SageMaker, please see \n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
